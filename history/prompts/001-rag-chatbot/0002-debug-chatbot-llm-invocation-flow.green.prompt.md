---
id: "0002"
title: Debug chatbot LLM invocation flow
stage: green
date: 2025-12-21
surface: agent
model: claude-opus-4-5-20251101
feature: 001-rag-chatbot
branch: 001-rag-chatbot
user: muhammad-yousuf
command: /sp.implement
labels: ["debugging", "logging", "gemini", "rag"]
links:
  spec: null
  ticket: null
  adr: null
  pr: null
files:
  - src/api/chat.py
  - src/services/agent_service.py
  - src/agents/rag_agent.py
  - src/services/gemini_client.py
  - src/services/rag_service.py
tests:
  - null
---

## Prompt

You are a senior backend engineer and debugging agent.

GOAL:
My FastAPI-based RAG chatbot backend is running without errors, but the chatbot does not respond.
The server starts, database works, health checks pass, but no LLM response is generated.

TASKS:
1. Analyze the backend codebase starting from the chat API endpoint.
2. Trace the full request flow:
   - HTTP request
   - conversation creation
   - message retrieval
   - RAG retrieval (Qdrant)
   - LLM invocation (OpenAI or Gemini)
   - response streaming or return
3. Identify where execution stops or returns early.
4. Detect logic blockers such as:
   - early return before LLM call
   - incorrect condition on FULL_BOOK vs SELECTED_TEXT
   - missing or empty prompt construction
   - swallowed exceptions (`except: pass`)
   - streaming generator that never yields
5. Verify that:
   - LLM is actually invoked
   - the response is returned or streamed to the client
6. Fix the issue directly in code.

CONSTRAINTS:
- Do NOT change database schema.
- Do NOT remove RAG functionality.
- Do NOT change frontend code.
- Keep current API routes and response format.
- Log errors instead of silently ignoring them.

REQUIRED ACTIONS:
- Add clear logging before and after the LLM call.
- Ensure at least one assistant message is generated and returned.
- Ensure streaming responses yield tokens if streaming is used.
- If logic branches block execution, correct them.

OUTPUT:
1. Show the exact file(s) and line(s) where the bug existed.
2. Apply the fix.
3. Explain briefly (2â€“3 lines) why the chatbot was not responding.
4. Confirm how the fix ensures the LLM is now called.

IMPORTANT:
The backend is already running; this is a logic-flow bug, not an infrastructure or dependency issue.
Solve it accordingly.

## Response snapshot

Traced the full request flow from chat API through agent service, RAG agent, and Gemini client. Identified that errors were being silently swallowed in exception handlers without logging. Added comprehensive logging throughout the request pipeline:

1. **chat.py**: Added logging for request/response and exposed actual error details
2. **agent_service.py**: Added logging for conversation, RAG retrieval, and LLM calls
3. **rag_agent.py**: Added logging before/after Gemini API calls with error handling
4. **gemini_client.py**: Added logging for message conversion and API calls
5. **rag_service.py**: Added logging for embedding creation and Qdrant search

The fix ensures all errors are now visible and traceable.

## Outcome

- Impact: Added comprehensive error logging throughout the chat request pipeline
- Tests: Manual testing required - restart backend and send a chat message
- Files: 5 files modified with logging additions
- Next prompts: Test the chatbot and verify logs show the error source
- Reflection: Silent exception handling is a common anti-pattern that makes debugging impossible

## Evaluation notes (flywheel)

- Failure modes observed: Silent exception swallowing in try/except blocks
- Graders run and results (PASS/FAIL): N/A - logging fix, manual verification needed
- Prompt variant (if applicable): null
- Next experiment (smallest change to try): Check logs after restart to identify actual error
